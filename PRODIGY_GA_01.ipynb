{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCMWHKYdDZKpwcRuVG1LyU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6ccabe6cb0d4076a35524ed33c4aba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b702bc000010403a8a4675e61c6b7005",
              "IPY_MODEL_921c60559f3948b4818f0eb66502b964",
              "IPY_MODEL_09019d56967844db9499f312583bee7c"
            ],
            "layout": "IPY_MODEL_8e5116f7b71642b3aa51acf718c1ac75"
          }
        },
        "b702bc000010403a8a4675e61c6b7005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c674ae2978e34a3b9abf62405736b731",
            "placeholder": "​",
            "style": "IPY_MODEL_ccc9e837a115468ba4356d02095b9839",
            "value": "Map: 100%"
          }
        },
        "921c60559f3948b4818f0eb66502b964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4ab65956df44a429133ed11ae94d2ca",
            "max": 6844,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c972f1e7c524adab768d5bb8ae613ab",
            "value": 6844
          }
        },
        "09019d56967844db9499f312583bee7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49e78892fb714f3db1069705f69ccedc",
            "placeholder": "​",
            "style": "IPY_MODEL_ec1376cef2d74bfeafba4016a272c943",
            "value": " 6844/6844 [00:15&lt;00:00, 340.29 examples/s]"
          }
        },
        "8e5116f7b71642b3aa51acf718c1ac75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c674ae2978e34a3b9abf62405736b731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccc9e837a115468ba4356d02095b9839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4ab65956df44a429133ed11ae94d2ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c972f1e7c524adab768d5bb8ae613ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49e78892fb714f3db1069705f69ccedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec1376cef2d74bfeafba4016a272c943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharvarijiwtode/PRODIGY_GA/blob/main/PRODIGY_GA_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rOtlfOQ1xEL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ea1890-4d17-4984-9d70-cbaec33aea66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m456.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: torchaudio==2.0.2 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (11.2.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-2.0.1+cu118\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#STEP-01\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print( torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)\n",
        "print(torch.backends.cudnn.version())"
      ],
      "metadata": {
        "id": "XWXvn36Ldy6H",
        "outputId": "efff5b63-1320-4cf7-c0fa-56aa45bb7349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "True\n",
            "11.8\n",
            "8700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "6SGVmD4qemim",
        "outputId": "5d6e1a43-9e30-4ce8-c22e-7281321bb54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.3)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m454.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-2.0.1+cu118\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvfuser",
                  "torch"
                ]
              },
              "id": "4e016a284ee14ff7bb7647187d936c6d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP-02\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/Articles.csv', encoding = 'ISO-8859-1')\n",
        "print(df.columns)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "SXdnjn8fz6Yf",
        "outputId": "32630c5a-45ad-4334-94f9-5eed6f8a6dd6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Article', 'Date', 'Heading', 'NewsType'], dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Article      Date  \\\n",
              "0  KARACHI: The Sindh government has decided to b...  1/1/2015   \n",
              "1  HONG KONG: Asian markets started 2015 on an up...  1/2/2015   \n",
              "2  HONG KONG:  Hong Kong shares opened 0.66 perce...  1/5/2015   \n",
              "3  HONG KONG: Asian markets tumbled Tuesday follo...  1/6/2015   \n",
              "4  NEW YORK: US oil prices Monday slipped below $...  1/6/2015   \n",
              "\n",
              "                                             Heading  NewsType  \n",
              "0  sindh govt decides to cut public transport far...  business  \n",
              "1                    asia stocks up in new year trad  business  \n",
              "2           hong kong stocks open 0.66 percent lower  business  \n",
              "3             asian stocks sink euro near nine year   business  \n",
              "4                 us oil prices slip below 50 a barr  business  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21851a05-205f-48b2-b4e6-96cf39a2d183\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Date</th>\n",
              "      <th>Heading</th>\n",
              "      <th>NewsType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KARACHI: The Sindh government has decided to b...</td>\n",
              "      <td>1/1/2015</td>\n",
              "      <td>sindh govt decides to cut public transport far...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HONG KONG: Asian markets started 2015 on an up...</td>\n",
              "      <td>1/2/2015</td>\n",
              "      <td>asia stocks up in new year trad</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HONG KONG:  Hong Kong shares opened 0.66 perce...</td>\n",
              "      <td>1/5/2015</td>\n",
              "      <td>hong kong stocks open 0.66 percent lower</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HONG KONG: Asian markets tumbled Tuesday follo...</td>\n",
              "      <td>1/6/2015</td>\n",
              "      <td>asian stocks sink euro near nine year</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NEW YORK: US oil prices Monday slipped below $...</td>\n",
              "      <td>1/6/2015</td>\n",
              "      <td>us oil prices slip below 50 a barr</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21851a05-205f-48b2-b4e6-96cf39a2d183')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21851a05-205f-48b2-b4e6-96cf39a2d183 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21851a05-205f-48b2-b4e6-96cf39a2d183');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c79fe1aa-2b9b-4a61-bc92-873f185552c2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c79fe1aa-2b9b-4a61-bc92-873f185552c2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c79fe1aa-2b9b-4a61-bc92-873f185552c2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2692,\n  \"fields\": [\n    {\n      \"column\": \"Article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2584,\n        \"samples\": [\n          \"SEOUL: South Korean swimming star Park Tae-Hwan, who was barred from the Olympics for doping, will compete for the national team in Rio after the Court of Arbitration for Sports (CAS) ruled in his favour.The multiple Olympic medallist had sought \\\"an urgent ruling\\\" from the Lausanne-based body against his doping ban by July 8 -- the deadline for South Korea to select their Rio swimming team.\\\"We will include Park Tae-Hwan in the list of national athletes who will take part in the Rio Olympics\\\", Korean Olympic Committee (KOC) Secretary General Cho Young-Ho was quoted as saying by Yonhap news agency.Park, 26, was slapped with an 18-month suspension after testing positive for an anabolic steroid in out-of-competition controls before the 2014 Asian Games.The ban lapsed in April, but Park remained barred from the 2016 Olympics under a KOC rule which prohibits athletes from representing the country for three years after a doping ban expires.CAS ruled Friday that KOC\\u00b4s extended ban constituted a double jeopardy for Park and should be nullified, according to KOC.South Korea\\u00b4s Olympic committee had said earlier Friday it would abide by the CAS ruling.Park -- once the poster boy of South Korean swimming before the doping scandal -- has repeatedly begged for a chance to compete in what would be his third, and probably last, Olympics.He won 400m freestyle gold and 200m freestyle silver at the 2008 Beijing Olympics and two silver medals at the 2012 London Olympics, as well as 400m world titles in 2007 and 2011.\\u00a0\",\n          \"MEXICO: Popular burrito chain Chipotle Mexican Grill Inc will close its restaurants for a few hours next month to hold a meeting on food safety with employees, company executives said.Chipotle, which has been plagued by a series of food poisoning outbreaks, will hold the meeting on Feb.8, the executives said at the ICR Conference on Wednesday.Chipotle is confident that steps being taken to tighten food safety will prevent future food poisoning outbreaks, the executives said.\\u00a0\",\n          \"strong>Martina Hingis and Sania Mirza lost their cool as their hopes of holding all four grand slam titles were dashed following a 6-3 6-2 defeat by Czech duo Barbora Krejcikova and Katerina Siniakova in the third round of the French Open on Sunday.</strongThe Indo-Swiss team dubbed \\\"Santina\\\" were hoping to win four majors in a row after following up last year's Wimbledon and U.S. Open triumphs with victory in January's Australian Open.But the top seeds appeared all at sea on a gloomy day at Roland Garros and their bid to complete the 'Santina Slam' ended after the Czechs blasted a service return winner to break Hingis in the final game.Hingis and Mirza were left fuming at the umpire after a call went against them as they trailed 4-1 in the second set. Mirza tossed the ball in anger but the duo failed to win the argument, or the match, leaving Krejcikova and Siniakova to celebrate a memorable victory.It proved to be a bad day for the big names in doubles as Venus and Serena Williams, who were the last women's team to hold all four majors in 2010, also perished in the third round, beaten 6-3 6-3 by the Dutch-Swedish pairing of Kiki Bertens and Johanna Larsson.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 666,\n        \"samples\": [\n          \"1/15/2017\",\n          \"8/10/2015\",\n          \"2/7/2017\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Heading\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2581,\n        \"samples\": [\n          \"PIA signs contact lease three A 330s Sri L\",\n          \"Kohli masterclass prompts Tendulkar compari\",\n          \"Pound tests new 31 year low vs dollar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NewsType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"sports\",\n          \"business\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('NewsType').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "q_oC-q_LGmJZ",
        "outputId": "68de39f6-8a0b-4a66-b552-8c69310d2681"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP-03\n",
        "df['Article'] = df['Article'].astype(str)\n",
        "df['Article'].to_csv('train_data.txt', index=False, header=False)"
      ],
      "metadata": {
        "id": "K_U6GNYI1Ytn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP-04\n",
        "from datasets import Dataset\n",
        "with open(\"train_data.txt\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "  lines = f.readlines()\n",
        "dataset = Dataset.from_dict({\"text\":lines})\n",
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW2RbjwE1z_4",
        "outputId": "17656906-6005-4d7b-8ff6-9abf006ecc08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '\"KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum product prices by the federal government, Geo News reported.Sources said reduction in fares will be applicable on public transport, rickshaw, taxi and other means of traveling.Meanwhile, Karachi Transport Ittehad (KTI) has refused to abide by the government decision.KTI President Irshad Bukhari said the commuters are charged the lowest fares in Karachi as compare to other parts of the country, adding that 80pc vehicles run on Compressed Natural Gas (CNG). Bukhari said Karachi transporters will cut fares when decrease in CNG prices will be made.                        \\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch -y\n",
        "!pip cache purge"
      ],
      "metadata": {
        "id": "IDBsFyTFY80i",
        "outputId": "99e4666f-a3fe-479a-ab48-45414633fa9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.1+cu118\n",
            "Uninstalling torch-2.0.1+cu118:\n",
            "  Successfully uninstalled torch-2.0.1+cu118\n",
            "Files removed: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP-05\n",
        "from transformers import GPT2Tokenizer\n",
        "from datasets import Dataset\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(batch):\n",
        "  tokenized = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "  tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "  return tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "_kj2jJgu21ZQ",
        "outputId": "2c8acd3b-e923-4a78-b892-90e7f80f7b41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'torch.sparse._triton_ops'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlignDevicesHook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_hook_to_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m from .big_modeling import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcheckpointing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_custom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_custom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoaderDispatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_first_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/checkpointing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_model_to_fp8_ao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_first_and_last_linear_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_ao_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m from .constants import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/ao.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torchao_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat8_linear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloat8LinearConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from torchao.quantization import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mautoquant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from torchao.kernel import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mint_scaled_matmul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msafe_int_mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/kernel/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbsr_triton_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbsr_dense_addmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mint_scaled_matmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_int_mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/kernel/bsr_triton_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mwarn_once\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from torch.sparse._triton_ops import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbroadcast_batch_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.sparse._triton_ops'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4f982541642a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2063\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0;31m# Third attempt. If we have not yet found the original type of the tokenizer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m                 \u001b[0;31m# we are loading we see if we can infer it from the type of the configuration file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m                 \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTOKENIZER_MAPPING_NAMES\u001b[0m  \u001b[0;31m# tests_ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoderConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m from .configuration_auto import (\n\u001b[1;32m     40\u001b[0m     \u001b[0mCONFIG_MAPPING_NAMES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'torch.sparse._triton_ops'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "print(tokenized_dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e6ccabe6cb0d4076a35524ed33c4aba8",
            "b702bc000010403a8a4675e61c6b7005",
            "921c60559f3948b4818f0eb66502b964",
            "09019d56967844db9499f312583bee7c",
            "8e5116f7b71642b3aa51acf718c1ac75",
            "c674ae2978e34a3b9abf62405736b731",
            "ccc9e837a115468ba4356d02095b9839",
            "d4ab65956df44a429133ed11ae94d2ca",
            "8c972f1e7c524adab768d5bb8ae613ab",
            "49e78892fb714f3db1069705f69ccedc",
            "ec1376cef2d74bfeafba4016a272c943"
          ]
        },
        "id": "SLPIQ44C-mCJ",
        "outputId": "1265a1ff-829a-4b6d-ba09-9249f7791fc1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6844 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6ccabe6cb0d4076a35524ed33c4aba8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['text', 'input_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbNu8S78_Zn4",
        "outputId": "b21ec6be-b646-4a90-cae5-bc35e97b7c01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP-06\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch, os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gta2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "dataset = load_dataset(\"trin_data.txt\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(batch):\n",
        "    tokenized = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize and reduce dataset size\n",
        "tokenized_dataset = dataset[\"train\"].map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.select(range(500))  # Limit for speed\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    max_steps=100,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    logging_steps=10,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "h54TicCH32MO",
        "outputId": "da104d34-e93a-48c0-c8e2-7a011b117a8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.training_args because of the following error (look up to see its traceback):\nNo module named 'torch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAcceleratorState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartialState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m from .big_modeling import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-72c271b07f60>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#STEP-06\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.training_args because of the following error (look up to see its traceback):\nNo module named 'torch'"
          ]
        }
      ]
    }
  ]
}